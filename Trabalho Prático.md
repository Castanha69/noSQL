# Trabalho pr√°tico de Banco de Dados noSQL
## Coletar informa√ß√µes de redes sociais e armazenar no m√≠nimo 1MM de dados.
### Extrair informa√ß√µes do tipo:

1. Termos mais frequentes:
2. Volume por dia:
3. Volume por hora do dia:


###[1] **termos mais frequentes:**
Escolhemos a rede social TWITTER como objeto de nosso trabalho, por j√° termos tido uma experi√™ncia inicial com a mesma em aula.
Para a captura dos dados, foi desenvolvido um c√≥digo em Python para capturar tweets e em seguida armazen√°-los. Como efeito de estudo armazenamos estes dados em dois formatos: arquivo json e dentro do Banco de Dados MongoDB para que pud√©ssemos praticar os dois m√©todos.

Um segundo c√≥digo foi gerado para acessar os dados armazenados e ent√£o fazer a identifica√ß√£o dos temos mais frequentes.
As linguagens utilizadas foram Python e Java Script, sendo que a segunda era necess√°ria para trabalharmos os dados no MongoDB.


#### Acesso ao Twitter
Para termos acesso ao Twitter, tivemos que primeiramente criar uma conta e em seguida criar um aplicativo para ent√£o termos condi√ß√µes de escutarmos o que era "twitado" na internet. Para evitarmos os bloqueios que poder√≠amos sofrer, criamos dois aplicativos, de forma a burlar o "xerife" do Twitter. Assim, de tempos em tempos alter√°vamos o aplicativos que utiliz√°vamos.
O primeiro aplicativo se chamava [**TweetyMining69**](https://apps.twitter.com/app/13191380). 
O segundo aplicativo se chamava [**tweety mining**](https://apps.twitter.com/app/13170132/show)

#### Defini√ß√£o de Armazenamento
Conforme mencionado anteriormente utilizamos dois m√©todos distintos de armazenamento, sendo o primeiro um arquivo texto, com extens√£o json e o segundo um arquivo no banco de dados MongoDB.
Arquivo Json: with open("c:\\Users\\Marcelo\\Documents\\noSQL-twt.json",'a') as file:

MongoDB: 
```python
client = pymongo.MongoClient('localhost', 27017)
db = client.tweet_database
collection = db.tweet_collection
```

O Armazenamento dos dados em ambos os ambientes √© muito simples e f√°cil de fazer.
Armazenamos os dados no arquivo texto, em formato json, para que os dados j√° ficassem estruturados e assim pud√©ssemos depois trabalhar com os mesmos. No caso do MongoDB isto √© mais simples, pois ele sozinho j√° arquiva os dados em formato Json.
Como √© de se esperar nesta fase n√£o h√° diferen√ßa, percept√≠vel, de velocidade entre uma atividade e a outra.

O armazenamento no arquivo de texto √© feito pelo comando: __file.write(testdata2)__ sendo testdata2 os dados do twitter capturado.
J√° no caso do MongoDB, √© feito pelo comando: __tweetind = collection.insert_one(testdata2).inserted_id__.

### PALAVRAS DE BUSCA
Para este trabalho pr√°tico escolhemos palavras de busca voltadas para o momento em que estamos vivendo - Natal, Fim de Ano, Festas.
Assim as palavras selecionadas foram:
```python
Words_to_Track = ['Natal','felicidade','amor','paz','alegria','√°rvore','fam√≠lia','uni√£o',
                  'champagne','luz','tren√≥','presentes','beijos','abra√ßos','renas',
                  'papai noel','saco de presentes',
                  'bebedeira','festa de fim de ano','fim de ano','sexo','camisinha','suruba',
                  'ano novo', 'dinheiro']
```                  
                  
Vemos que o range de palavras variam bastante indo de uma no√ß√£o religiosa crist√£ √† termos mais associados a festas pag√£s. Com isto busc√°vamos abordar os diferentes grupos de indiv√≠duos que se comunicaram durante o per√≠odo.


### A COLETA E TRATAMENTO dos TWEETS
A coleta dos tweets √© feita atrav√©s do acesso ao Twitter, pela aplica√ß√£o que foi criada inicialmente no twitter.
Quando criamos a aplica√ß√£o no twitter chaves e tokens de acesso s√£o criados para que apenas o dono da aplica√ß√£o possa fazer uso da mesma.
Ex.:  
 ConsumerKey (API Key)        -> rPQLXBYOnk3TbKgzX2Qe8TfJx  
 `Consumer Secret (API Secret) -> t31Pi0v4lJfMaZ4H9csEqt2uUwotiTDLC7waxHAnAWtHA3Uf3A`  
 Access Token                 -> 805224390954782720-eLwQWp5C7ncpmgqUKWvZ72Y1hdgtjYE  
 `Access Token Secret          -> RdSsAiXmSRh1lEMeMLX0lBUdnocH73P0aEZnuRgBWtUJ8`  
 
Assim temos de passar tokens e dados de acesso a aplica√ß√£o para que nosso acesso seja liberado.
Isto √© feito enviando via APIs ao Twitter os dados acima:
```python
consumer_Key = "rPQLXBYOnk3TbKgzX2Qe8TfJx"
consumer_Secret = "t31Pi0v4lJfMaZ4H9csEqt2uUwotiTDLC7waxHAnAWtHA3Uf3A"
access_Token = "805224390954782720-eLwQWp5C7ncpmgqUKWvZ72Y1hdgtjYE"
access_Secret = "RdSsAiXmSRh1lEMeMLX0lBUdnocH73P0aEZnuRgBWtUJ8"

auth = OAuthHandler(consumer_Key, consumer_Secret)
auth.set_access_token(access_Token, access_Secret)
stream = Stream(auth, l)
```
Ap√≥s isto podemos come√ßar a coletar os twitters com o comando abaixo:  
```python
stream.filter(track=Words_to_Track)
```
Atrav√©s do comando stream.filter iremos coletar todos os twitters que contenham as palavras que definimos anteriormente. Um fato interessante √© que dependendo da hora e das palavras a coleta ser√° r√°pida ou lenta.

Um fato que observamos ap√≥s as coletas √© que forma coletados twitters de todas as linguas, que contivessem alguma das palavras de busca. Assim foram coletados twitters em portugu√™s, ingl√™s e espanhol. Isto ir√° afetar, mais a frente, o tratamento das frequ√™ncias dos termos, pois teremos tr√™s idiomas.

#### Tratamento dos Twitters
Aproveitamos para j√° dar um tratamento aos twitters t√£o logo os captur√°ssemos, atuando sobre o campo 'texto' que √© a mensagem que √© tuitada e j√° a dividindo em termos independentes e n√£o mais como uma frase. Isto quer dizer, desconsideramos as pontua√ß√µes e outros caracteres.
Fizemos isto com o intuito de ganhar tempo j√° que o nosso objetivo neste trabalho pr√°tico √© determinar com que frequ√™ncia alguns termos s√£o utilizados.
Assim, t√£o logo capturavamos o twitter, j√° transform√°vamos o texto em TOKENS, ou melhor, uma lista de palavras. Isto era feito com o comando:
```python
tokens = process(text=tweet.get('text', ''),
                             tokenizer=tweet_tokenizer,
                             stopwords=stopword_list)
```
Para esta tarefa de transformar em TOKENS o texto utilizamos os pacotes NLTK, que √© uma plataforma l√≠der para construir programas em Python para trabalhar com dados de linguagem humana.

O c√≥digo completo pode ser visto no arquivo Coleta_Tweets.py.  

Para efeito de estudo, fizemos duas coletas distintas de twitters, sendo uma coletando e armazenando os twitters sem tratamento e a segunda com o texto j√° transformado em tokens. Assim foram criados no MongoDB duas cole√ß√µes: **tweet_collection** e **tweet_coll_token** respectivamente.  


### IDENTIFICA√á√ÉO DOS TERMOS MAIS FREQUENTES.
Para ent√£o tratarmos os twitters que foram coletados foram desenvolvidos dois c√≥digos distintos, sendo um em Python e o segundo em Java Script, para rodar direto do MongoDB, ou seja, utilizando as "engines" do banco de dados.

#### 1. C√≥digo Python
Nesta fase, n√£o mais precisamos acessar o Twitter, logo s√≥ acessamos o MongoDB:
```python
    client = pymongo.MongoClient('localhost', 27017)
    db = client.tweet_database
    collection = db.tweet_collection
```
Como v√™, estamos utilizando a cole√ß√£o que contem o twitter puro, com seus campos intactos, por isto, fizemos a tokeniza√ß√£o do texto na hora de contar os termos. Esta contagem √© feita utilizando-se um contador, count_all = Counter().

Buscando um conhecimento melhor do sentido que as palavras teriam, utilizamos a fun√ß√£o "bigrams" para contar dois tokens que aparecam juntos na mesma frase, desta forma conseguimos ter alguma id√©ia do que significado do twitter.

No c√≥digo contamos os termos da sequinte forma:
```python
        terms_all = [term for term in tokens]
        terms_bigram = bigrams(terms_all) #juntei dois tokens dos mesmos tweets, buscando mais sentido.
        # Update the counter
        count_all.update(terms_bigram)
```
Por INCR√çVEL que pare√ßa, o c√≥digo em Python rodou com sucesso, por√©m demorou mais de uma hora para chegar ao fim.
O resultado desta an√°lise foi a seguinte:  
[(('el', 'amor'), 73273),  
(('mi', 'amor'), 31323),  
(('‚ù§', 'Ô∏è'), 26471),  
(('en', 'el'), 26029),  
(('amor', 'y'), 18267),  
(('amor', 'es'), 15639),  
(('üòÇ', 'üòÇ'), 13190),  
(('un', 'amor'), 12408),  
(('jesus', 'christ'), 12024),  
(('amor', 'vida'), 11841),  
(('amor', 'deus'), 11255)]

Um fato interessante a se notar aqui √© que as palavras mais "pagans" ou de baixo cal√£o, n√£o aparecem entre as 10 primeiras. Logo, podemos inferir que as pessoas neste per√≠odo em que estamos d√£o mais valor ao sentido religioso e a uni√£o de suas fam√≠lias.


#### 2. C√≥digo Java Script - MongoDB
Em um segundo momento fizemos a contagem dos termos mais frequentes utilizando a pr√≥pria "engine" do MongoDB, que nos d√° muita performance.
Um ponto importante aqui: √â necess√°rio criar-se um √çNDICE antes para que o MongoDB possa operar com performance.
Outro ponto importante √© que o c√≥digo que roda no MongoDB est√° em JAVA SCRIPT.

Assim primeiramente no Shell do MongoDB criamos o √çndice:
```js
db.tweet_coll_token.createIndex({'text':1})
```
Para fazermos a identifica√ß√£o dos termos utilizamos ent√£o a fun√ß√£o de mapReduce do MongoDB que permite identificar os termos e som√°-los.
```js
var map = function() {  
    var text = this.text;
    if (text) { 
        // quick lowercase to normalize per your requirements
        // text = text.toLowerCase(); 
        for (var i = text.length - 1; i >= 0; i--) {
            if (text[i])  {      // make sure there's something
               emit(text[i], 1); // store a 1 for each word
            }
        }
    }
};

var reduce = function( key, values ) {    
    var count = 0;    
    values.forEach(function(v) {            
        count +=v;    
    });
    return count;
}


db.tweet_coll_token.mapReduce(map, reduce, {out: "word_count"})

db.word_count.find().sort({value:-1})
```

Como v√™em utilizamos a collection tweet_coll_token pois ela j√° estava tratada e n√£o continga pontua√ß√£o e seus termos j√° estavam em forma de token.

O mesmo procedimento que demorou mais de uma hora para ser conclu√≠do no Python, dentro do MongoDB demorou menos de 05 minutos.

O resultado foi o seguinte:
```js
> db.word_count.find().limit(100).sort({'value':-1})
{ "_id" : "amor", "value" : 461860 }
{ "_id" : "deus", "value" : 193588 }
{ "_id" : "‚Ä¶", "value" : 177886 }
{ "_id" : "el", "value" : 161307 }
{ "_id" : "√©", "value" : 146218 }
{ "_id" : "jesus", "value" : 131832 }
{ "_id" : "y", "value" : 123877 }
{ "_id" : "la", "value" : 90653 }
{ "_id" : "en", "value" : 84001 }
{ "_id" : "oi", "value" : 83701 }
{ "_id" : "...", "value" : 83104 }
{ "_id" : "es", "value" : 73122 }
{ "_id" : "mi", "value" : 70555 }
{ "_id" : "natal", "value" : 65515 }
{ "_id" : "to", "value" : 65463 }
{ "_id" : "pra", "value" : 63406 }
{ "_id" : "‚ù§", "value" : 60484 }
{ "_id" : "the", "value" : 60424 }
{ "_id" : "q", "value" : 52856 }
{ "_id" : "vida", "value" : 51512 }
```






