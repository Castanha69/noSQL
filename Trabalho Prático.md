# Trabalho pr√°tico de Banco de Dados noSQL
## Coletar informa√ß√µes de redes sociais e armazenar no m√≠nimo 1MM de dados.
### Extrair informa√ß√µes do tipo:

1. Termos mais frequentes:
2. Volume por dia:
3. Volume por hora do dia:


###[1] **termos mais frequentes:**
Escolhemos a rede social TWITTER como objeto de nosso trabalho, por j√° termos tido uma experi√™ncia inicial com a mesma em aula.
Para a captura dos dados, foi desenvolvido um c√≥digo em Python para capturar tweets e em seguida armazen√°-los. Como efeito de estudo armazenamos estes dados em dois formatos: arquivo json e dentro do Banco de Dados MongoDB para que pud√©ssemos praticar os dois m√©todos.

Um segundo c√≥digo foi gerado para acessar os dados armazenados e ent√£o fazer a identifica√ß√£o dos temos mais frequentes.
As linguagens utilizadas foram Python e Java Script, sendo que a segunda era necess√°ria para trabalharmos os dados no MongoDB.


#### Acesso ao Twitter
Para termos acesso ao Twitter, tivemos que primeiramente criar uma conta e em seguida criar um aplicativo para ent√£o termos condi√ß√µes de escutarmos o que era "twitado" na internet. Para evitarmos os bloqueios que poder√≠amos sofrer, criamos dois aplicativos, de forma a burlar o "xerife" do Twitter. Assim, de tempos em tempos alter√°vamos o aplicativos que utiliz√°vamos.
O primeiro aplicativo se chamava [**TweetyMining69**](https://apps.twitter.com/app/13191380). 
O segundo aplicativo se chamava [**tweety mining**](https://apps.twitter.com/app/13170132/show)

#### Defini√ß√£o de Armazenamento
Conforme mencionado anteriormente utilizamos dois m√©todos distintos de armazenamento, sendo o primeiro um arquivo texto, com extens√£o json e o segundo um arquivo no banco de dados MongoDB.
Arquivo Json: with open("c:\\Users\\Marcelo\\Documents\\noSQL-twt.json",'a') as file:

MongoDB: 
```python
client = pymongo.MongoClient('localhost', 27017)
db = client.tweet_database
collection = db.tweet_collection
```

O Armazenamento dos dados em ambos os ambientes √© muito simples e f√°cil de fazer.
Armazenamos os dados no arquivo texto, em formato json, para que os dados j√° ficassem estruturados e assim pud√©ssemos depois trabalhar com os mesmos. No caso do MongoDB isto √© mais simples, pois ele sozinho j√° arquiva os dados em formato Json.
Como √© de se esperar nesta fase n√£o h√° diferen√ßa, percept√≠vel, de velocidade entre uma atividade e a outra.

O armazenamento no arquivo de texto √© feito pelo comando: __file.write(testdata2)__ sendo testdata2 os dados do twitter capturado.
J√° no caso do MongoDB, √© feito pelo comando: __tweetind = collection.insert_one(testdata2).inserted_id__.

### PALAVRAS DE BUSCA
Para este trabalho pr√°tico escolhemos palavras de busca voltadas para o momento em que estamos vivendo - Natal, Fim de Ano, Festas.
Assim as palavras selecionadas foram:
```python
Words_to_Track = ['Natal','felicidade','amor','paz','alegria','√°rvore','fam√≠lia','uni√£o',
                  'champagne','luz','tren√≥','presentes','beijos','abra√ßos','renas',
                  'papai noel','saco de presentes',
                  'bebedeira','festa de fim de ano','fim de ano','sexo','camisinha','suruba',
                  'ano novo', 'dinheiro']
```                  
                  
Vemos que o range de palavras variam bastante indo de uma no√ß√£o religiosa crist√£ √† termos mais associados a festas pag√£s. Com isto busc√°vamos abordar os diferentes grupos de indiv√≠duos que se comunicaram durante o per√≠odo.


### A COLETA E TRATAMENTO dos TWEETS
A coleta dos tweets √© feita atrav√©s do acesso ao Twitter, pela aplica√ß√£o que foi criada inicialmente no twitter.
Quando criamos a aplica√ß√£o no twitter chaves e tokens de acesso s√£o criados para que apenas o dono da aplica√ß√£o possa fazer uso da mesma.
Ex.:  
 ConsumerKey (API Key)        -> rPQLXBYOnk3TbKgzX2Qe8TfJx  
 `Consumer Secret (API Secret) -> t31Pi0v4lJfMaZ4H9csEqt2uUwotiTDLC7waxHAnAWtHA3Uf3A`  
 Access Token                 -> 805224390954782720-eLwQWp5C7ncpmgqUKWvZ72Y1hdgtjYE  
 `Access Token Secret          -> RdSsAiXmSRh1lEMeMLX0lBUdnocH73P0aEZnuRgBWtUJ8`  
 
Assim temos de passar tokens e dados de acesso a aplica√ß√£o para que nosso acesso seja liberado.
Isto √© feito enviando via APIs ao Twitter os dados acima:
```python
consumer_Key = "rPQLXBYOnk3TbKgzX2Qe8TfJx"
consumer_Secret = "t31Pi0v4lJfMaZ4H9csEqt2uUwotiTDLC7waxHAnAWtHA3Uf3A"
access_Token = "805224390954782720-eLwQWp5C7ncpmgqUKWvZ72Y1hdgtjYE"
access_Secret = "RdSsAiXmSRh1lEMeMLX0lBUdnocH73P0aEZnuRgBWtUJ8"

auth = OAuthHandler(consumer_Key, consumer_Secret)
auth.set_access_token(access_Token, access_Secret)
stream = Stream(auth, l)
```
Ap√≥s isto podemos come√ßar a coletar os twitters com o comando abaixo:  
```python
stream.filter(track=Words_to_Track)
```
Atrav√©s do comando stream.filter iremos coletar todos os twitters que contenham as palavras que definimos anteriormente. Um fato interessante √© que dependendo da hora e das palavras a coleta ser√° r√°pida ou lenta.

Um fato que observamos ap√≥s as coletas √© que forma coletados twitters de todas as linguas, que contivessem alguma das palavras de busca. Assim foram coletados twitters em portugu√™s, ingl√™s e espanhol. Isto ir√° afetar, mais a frente, o tratamento das frequ√™ncias dos termos, pois teremos tr√™s idiomas.

#### Tratamento dos Twitters
Aproveitamos para j√° dar um tratamento aos twitters t√£o logo os captur√°ssemos, atuando sobre o campo 'texto' que √© a mensagem que √© tuitada e j√° a dividindo em termos independentes e n√£o mais como uma frase. Isto quer dizer, desconsideramos as pontua√ß√µes e outros caracteres.
Fizemos isto com o intuito de ganhar tempo j√° que o nosso objetivo neste trabalho pr√°tico √© determinar com que frequ√™ncia alguns termos s√£o utilizados.
Assim, t√£o logo capturavamos o twitter, j√° transform√°vamos o texto em TOKENS, ou melhor, uma lista de palavras. Isto era feito com o comando:
```python
tokens = process(text=tweet.get('text', ''),
                             tokenizer=tweet_tokenizer,
                             stopwords=stopword_list)
```
Para esta tarefa de transformar em TOKENS o texto utilizamos os pacotes NLTK, que √© uma plataforma l√≠der para construir programas em Python para trabalhar com dados de linguagem humana.

O c√≥digo completo pode ser visto no arquivo Coleta_Tweets.py.  

Para efeito de estudo, fizemos duas coletas distintas de twitters, sendo uma coletando e armazenando os twitters sem tratamento e a segunda com o texto j√° transformado em tokens. Assim foram criados no MongoDB duas cole√ß√µes: **tweet_collection** e **tweet_coll_token** respectivamente.  


### IDENTIFICA√á√ÉO DOS TERMOS MAIS FREQUENTES.
Para ent√£o tratarmos os twitters que foram coletados foram desenvolvidos dois c√≥digos distintos, sendo um em Python e o segundo em Java Script, para rodar direto do MongoDB, ou seja, utilizando as "engines" do banco de dados.

#### 1. C√≥digo Python
Nesta fase, n√£o mais precisamos acessar o Twitter, logo s√≥ acessamos o MongoDB:
```python
    client = pymongo.MongoClient('localhost', 27017)
    db = client.tweet_database
    collection = db.tweet_collection
```
Como v√™, estamos utilizando a cole√ß√£o que contem o twitter puro, com seus campos intactos, por isto, fizemos a tokeniza√ß√£o do texto na hora de contar os termos. Esta contagem √© feita utilizando-se um contador, count_all = Counter().

Buscando um conhecimento melhor do sentido que as palavras teriam, utilizamos a fun√ß√£o "bigrams" para contar dois tokens que aparecam juntos na mesma frase, desta forma conseguimos ter alguma id√©ia do que significado do twitter.

No c√≥digo contamos os termos da sequinte forma:
```python
        terms_all = [term for term in tokens]
        terms_bigram = bigrams(terms_all) #juntei dois tokens dos mesmos tweets, buscando mais sentido.
        # Update the counter
        count_all.update(terms_bigram)
```
Por INCR√çVEL que pare√ßa, o c√≥digo em Python rodou com sucesso, por√©m demorou mais de uma hora para chegar ao fim.
O resultado desta an√°lise foi a seguinte:  
[(('el', 'amor'), 73273),  
(('mi', 'amor'), 31323),  
(('‚ù§', 'Ô∏è'), 26471),  
(('en', 'el'), 26029),  
(('amor', 'y'), 18267),  
(('amor', 'es'), 15639),  
(('üòÇ', 'üòÇ'), 13190),  
(('un', 'amor'), 12408),  
(('jesus', 'christ'), 12024),  
(('amor', 'vida'), 11841),  
(('amor', 'deus'), 11255)]

Um fato interessante a se notar aqui √© que as palavras mais "pagans" ou de baixo cal√£o, n√£o aparecem entre as 10 primeiras. Logo, podemos inferir que as pessoas neste per√≠odo em que estamos d√£o mais valor ao sentido religioso e a uni√£o de suas fam√≠lias.
O c√≥digo deste processo pode ser encontrado em **Term_Frequency.py**

#### 2. C√≥digo Java Script - MongoDB
Em um segundo momento fizemos a contagem dos termos mais frequentes utilizando a pr√≥pria "engine" do MongoDB, que nos d√° muita performance.
Um ponto importante aqui: √â necess√°rio criar-se um √çNDICE antes para que o MongoDB possa operar com performance.
Outro ponto importante √© que o c√≥digo que roda no MongoDB est√° em JAVA SCRIPT.

Assim primeiramente no Shell do MongoDB criamos o √çndice:
```js
db.tweet_coll_token.createIndex({'text':1})
```
Para fazermos a identifica√ß√£o dos termos utilizamos ent√£o a fun√ß√£o de mapReduce do MongoDB que permite identificar os termos e som√°-los.
```js
var map = function() {  
    var text = this.text;
    if (text) { 
        // quick lowercase to normalize per your requirements
        // text = text.toLowerCase(); 
        for (var i = text.length - 1; i >= 0; i--) {
            if (text[i])  {      // make sure there's something
               emit(text[i], 1); // store a 1 for each word
            }
        }
    }
};

var reduce = function( key, values ) {    
    var count = 0;    
    values.forEach(function(v) {            
        count +=v;    
    });
    return count;
}


db.tweet_coll_token.mapReduce(map, reduce, {out: "word_count"})

db.word_count.find().sort({value:-1})
```

Como v√™em utilizamos a collection tweet_coll_token pois ela j√° estava tratada e n√£o continga pontua√ß√£o e seus termos j√° estavam em forma de token.

O mesmo procedimento que demorou mais de uma hora para ser conclu√≠do no Python, dentro do MongoDB demorou **menos de 05 minutos**.

O resultado foi o seguinte:
```js
> db.word_count.find().limit(100).sort({'value':-1})
{ "_id" : "amor", "value" : 461860 }
{ "_id" : "deus", "value" : 193588 }
{ "_id" : "‚Ä¶", "value" : 177886 }
{ "_id" : "el", "value" : 161307 }
{ "_id" : "√©", "value" : 146218 }
{ "_id" : "jesus", "value" : 131832 }
{ "_id" : "y", "value" : 123877 }
{ "_id" : "la", "value" : 90653 }
{ "_id" : "en", "value" : 84001 }
{ "_id" : "oi", "value" : 83701 }
{ "_id" : "...", "value" : 83104 }
{ "_id" : "es", "value" : 73122 }
{ "_id" : "mi", "value" : 70555 }
{ "_id" : "natal", "value" : 65515 }
{ "_id" : "to", "value" : 65463 }
{ "_id" : "pra", "value" : 63406 }
{ "_id" : "‚ù§", "value" : 60484 }
{ "_id" : "the", "value" : 60424 }
{ "_id" : "q", "value" : 52856 }
{ "_id" : "vida", "value" : 51512 }
```
Infelizmente n√£o consegui fazer a mesma an√°lise de bigrams com o Js e MongoDB.


###[2] **VOLUME POR DIA:**

Para descobrirmos o volume de tweets por dia ter√≠amos de trabalhar com as datas e horas em que os twitters foram postados. Isto apresenta um novo desafio, por√©m pass√≠vel de solu√ß√£o.
A solu√ß√£o √© similar a anterior, por√©m ao inv√©s de termos similares, temos de encontrar dias similares.
Assim temos de trabalhar com o timestamp dos twitters.

Utilizando-se Python e a biblioteca PANDA esta tafefa fica muito f√°cil, pois a panda j√° traz muitos algor√≠tmos prontos para executar este tipo de tarefa. No entanto, devido ao grande tamanho do arquivo, a tarefa n√£o pode ser conclu√≠da utilizando-se este m√©todo, pois h√° consumo excessivo de mem√≥ria e por fim estouro, causando a parada do processo.

A solu√ß√£o √© trabalhar diretamente com o MongoDB que possui performance e engine para tal. Como temos de trabalhar com o tempo, temos de buscar os timestamps de cada twitter e somar os que s√£o do mesmo dia. Isto pode ser feito com a fun√ß√£o getTimestamp() e de novo utilizando-se a fun√ß√£o mapReduce do MongoDB.

Utilizamos a cole√ß√£o tweet_collection que cont√©m os twitters completos e sem tratamento, j√° que o que estamos buscando √© apenas o per√≠odo em que o mesmo foi postado.
Novamente, √© imprescind√≠vel criarmos um √çndice, pois sem o mesmo teremos um problema com o mapReduce.

```js
var map = function() {
    var datetime = this._id.getTimestamp();

    var created_at_Day = new Date(datetime.getFullYear(),
                                     datetime.getMonth(),
                                     datetime.getDate());
    emit(created_at_Day, {count: 1});
}

var reduce = function(key, values) {
    var total = 0;
    for(var i = 0; i < values.length; i++) { total += values[i].count; }
    return {count: total};
}

db.tweet_collection.mapReduce( map, reduce, { "out": "tweet_perDay" } );

db.tweet_perDay.find().limit(10).sort({"_id":-1})
```
Para a minha grande surpresa, esta pesquisa tomou menos de 1 minuto, o que aponta o poder do MongoDB em executar estas tarefas de mem√≥ria e CPU intensivas.
Segue o resultado:
```js
""" Resultado: """
> db.tweet_collection.mapReduce( map, reduce, { "out": "tweet_perDay" } );
{
        "result" : "tweet_perDay",
        "timeMillis" : 41389,
        "counts" : {
                "input" : 1294918,
                "emit" : 1294918,
                "reduce" : 12952,
                "output" : 3
        },
        "ok" : 1
}

> db.tweet_perDay.find().limit(10).sort({"_id":-1})
{ "_id" : ISODate("2016-12-**12**T02:00:00Z"), "value" : { "count" : 586075 } }  
{ "_id" : ISODate("2016-12-**11**T02:00:00Z"), "value" : { "count" : 687865 } }  
{ "_id" : ISODate("2016-12-**10**T02:00:00Z"), "value" : { "count" : 20978 } } 
```

Acredito que esta opera√ß√£o tamb√©m possa ser conseguida com a fun√ß√£o AGGREGATE, no entanto a Aggregate n√£o tem a fun√ß√£o de pegar o timestamp do twitter.

###[3] **VOLUME POR HORA:**
A solu√ß√£o √© igual a anterior. Novamente utilizaremos a collection tweet_collection.
O C√≥digo e resultados est√£o apresentados abaixo:
```js
var map = function() {
    var datetime = this._id.getTimestamp();

    var created_at_hour = new Date(datetime.getFullYear(),
                                     datetime.getMonth(),
                                     datetime.getDate(),
                                     datetime.getHours());
    emit(created_at_hour, {count: 1});
}

var reduce = function(key, values) {
    var total = 0;
    for(var i = 0; i < values.length; i++) { total += values[i].count; }
    return {count: total};
}

db.tweet_collection.mapReduce( map, reduce, { "out": "tweet_perHour" } );

db.tweet_perHour.find().limit(10).sort({"_id":-1})
```
#### Resultado:
```js
> db.tweet_collection.mapReduce( map, reduce, { "out": "tweet_perHour" } );  
{  
        "result" : "tweet_perHour",  
        "timeMillis" : 22658,  
        "counts" : {  
                "input" : 1294918,  
                "emit" : 1294918,  
                "reduce" : 12976,  
                "output" : 27  
             },  
        "ok" : 1  
        

> db.tweet_perHour.find().sort({"value":-1})
{ "_id" : ISODate("2016-12-12T02:00:00Z"), "value" : { "count" : 138047 } }
{ "_id" : ISODate("2016-12-12T03:00:00Z"), "value" : { "count" : 127166 } }
{ "_id" : ISODate("2016-12-11T02:00:00Z"), "value" : { "count" : 98605 } }
{ "_id" : ISODate("2016-12-12T04:00:00Z"), "value" : { "count" : 83084 } }
{ "_id" : ISODate("2016-12-11T19:00:00Z"), "value" : { "count" : 74931 } }
{ "_id" : ISODate("2016-12-11T18:00:00Z"), "value" : { "count" : 66356 } }
{ "_id" : ISODate("2016-12-11T21:00:00Z"), "value" : { "count" : 62524 } }
{ "_id" : ISODate("2016-12-11T20:00:00Z"), "value" : { "count" : 59585 } }
{ "_id" : ISODate("2016-12-11T17:00:00Z"), "value" : { "count" : 58072 } }
{ "_id" : ISODate("2016-12-11T14:00:00Z"), "value" : { "count" : 54621 } }
{ "_id" : ISODate("2016-12-12T05:00:00Z"), "value" : { "count" : 54373 } }
{ "_id" : ISODate("2016-12-12T00:00:00Z"), "value" : { "count" : 52339 } }
{ "_id" : ISODate("2016-12-11T04:00:00Z"), "value" : { "count" : 50691 } }
{ "_id" : ISODate("2016-12-12T10:00:00Z"), "value" : { "count" : 42768 } }
{ "_id" : ISODate("2016-12-11T03:00:00Z"), "value" : { "count" : 40784 } }
{ "_id" : ISODate("2016-12-12T06:00:00Z"), "value" : { "count" : 36294 } }
{ "_id" : ISODate("2016-12-12T09:00:00Z"), "value" : { "count" : 35908 } }
{ "_id" : ISODate("2016-12-11T10:00:00Z"), "value" : { "count" : 33995 } }
{ "_id" : ISODate("2016-12-12T08:00:00Z"), "value" : { "count" : 28108 } }
{ "_id" : ISODate("2016-12-12T07:00:00Z"), "value" : { "count" : 27482 } }
Type "it" for more
>
> it
{ "_id" : ISODate("2016-12-11T01:00:00Z"), "value" : { "count" : 20978 } }
{ "_id" : ISODate("2016-12-11T15:00:00Z"), "value" : { "count" : 19617 } }
{ "_id" : ISODate("2016-12-12T11:00:00Z"), "value" : { "count" : 12707 } }
{ "_id" : ISODate("2016-12-12T01:00:00Z"), "value" : { "count" : 9821 } }
{ "_id" : ISODate("2016-12-11T16:00:00Z"), "value" : { "count" : 5691 } }
{ "_id" : ISODate("2016-12-11T11:00:00Z"), "value" : { "count" : 233 } }
{ "_id" : ISODate("2016-12-12T13:00:00Z"), "value" : { "count" : 138 } }
```
### FONTES DE PESQUISA
Para executar este trabalho pr√°ticos foram utilizadas as seguintes fontes de pesquisa:  
a. Mining Twitter Data with Python - Bonzanini, Marco - (https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/) 

b. Twitter Data Analytics - Shamanth Kumar, Fred Morstatter, and Huan Liu - (http://tweettracker.fulton.asu.edu/tda/)  
c. StackOverflow - um √≥timo lugar para se buscar solu√ß√µes para problemas j√° conhecidos.  

As duas primeiras fontes de pesquisa s√£o muito boas, pois lhe municiam de conceitos o que lhe permite depois criar. J√° o StackOverflow √© um bom lugar para se buscar solu√ß√µes, c√≥digos ou novos algor√≠timos para desenvolver os conceitos que voc√™ aprendeu.

O item (a) tem foco apenas em Python e arquivos txt ou Json, no entanto √© muito rico no que tange a text mining, indo al√©m da frequ√™ncia de termos e entrando no sentimento dos textos. Muito interessante.
O item (b) tem j√° o foco sobre o MongoDB e tal como o primeiro lhe municia de muitos conceitos e tamb√©m vai muito al√©m da an√°lise simples de termos. 
